"""
DeepSeek AI å¯¹è¯åŠ©æ‰‹ + RAG çŸ¥è¯†åº“ - Streamlit åº”ç”¨
é˜¶æ®µ 1 + é˜¶æ®µ 2 å®Œæ•´ç‰ˆæœ¬
"""
import streamlit as st
from datetime import datetime
import os
from loguru import logger

from src.deepseek_client import DeepSeekClient
from src.embedding_handler import BGEEmbeddingHandler
from src.memory_kb_handler import MemoryKBHandler
from src.document_processor import DocumentProcessor
from src.rag_service import RAGService
from config import settings

# é…ç½®æ—¥å¿—
logger.add(
    settings.LOGS_DIR / "app.log",
    format="{time:YYYY-MM-DD HH:mm:ss} | {level: <8} | {message}",
    level=settings.LOG_LEVEL,
)

# é¡µé¢é…ç½®
st.set_page_config(
    page_title="DeepSeek AI Chat + RAG",
    page_icon="ğŸ’¬",
    layout="wide",
    initial_sidebar_state="expanded",
)

# è‡ªå®šä¹‰ CSS
st.markdown(
    """
    <style>
    .main {
        max-width: 1200px;
    }
    .stTabs [data-baseweb="tab-list"] button {
        width: 100%;
    }
    </style>
    """,
    unsafe_allow_html=True,
)


def initialize_session_state():
    """åˆå§‹åŒ– session state"""
    if "messages" not in st.session_state:
        st.session_state.messages = []

    if "deepseek_client" not in st.session_state:
        try:
            st.session_state.deepseek_client = DeepSeekClient()
        except ValueError as e:
            st.session_state.deepseek_client = None
            st.session_state.api_error = str(e)

    if "embedding_handler" not in st.session_state:
        try:
            logger.info("Loading BGE embedding model...")
            st.session_state.embedding_handler = BGEEmbeddingHandler()
            logger.info("BGE model loaded successfully")
        except Exception as e:
            logger.error(f"Failed to load BGE model: {str(e)}")
            st.session_state.embedding_handler = None
            st.session_state.embedding_error = str(e)

    if "kb_handler" not in st.session_state:
        try:
            if st.session_state.get("embedding_handler"):
                st.session_state.kb_handler = MemoryKBHandler(
                    st.session_state.embedding_handler
                )
            else:
                st.session_state.kb_handler = None
        except Exception as e:
            logger.error(f"Failed to initialize Knowledge Base: {str(e)}")
            st.session_state.kb_handler = None

    if "rag_service" not in st.session_state:
        try:
            if (
                st.session_state.get("embedding_handler")
                and st.session_state.get("kb_handler")
                and st.session_state.get("deepseek_client")
            ):
                st.session_state.rag_service = RAGService(
                    st.session_state.embedding_handler,
                    st.session_state.kb_handler,
                    st.session_state.deepseek_client,
                    top_k=5,
                )
            else:
                st.session_state.rag_service = None
        except Exception as e:
            logger.error(f"Failed to initialize RAG service: {str(e)}")
            st.session_state.rag_service = None

    if "document_processor" not in st.session_state:
        try:
            st.session_state.document_processor = DocumentProcessor(
                chunk_size=800, chunk_overlap=100
            )
        except Exception as e:
            logger.error(f"Failed to initialize document processor: {str(e)}")
            st.session_state.document_processor = None


def display_chat_history():
    """æ˜¾ç¤ºèŠå¤©å†å²"""
    for message in st.session_state.messages:
        with st.chat_message(message["role"]):
            st.markdown(message["content"])


def handle_user_input(user_input: str, use_rag: bool):
    """å¤„ç†ç”¨æˆ·è¾“å…¥"""
    # æ·»åŠ ç”¨æˆ·æ¶ˆæ¯åˆ°å†å²
    st.session_state.messages.append(
        {
            "role": "user",
            "content": user_input,
            "timestamp": datetime.now().isoformat(),
        }
    )

    # æ˜¾ç¤ºç”¨æˆ·æ¶ˆæ¯
    with st.chat_message("user"):
        st.markdown(user_input)

    # å¤„ç† AI å“åº”
    if st.session_state.deepseek_client is None:
        st.error("âŒ API é…ç½®é”™è¯¯ï¼Œè¯·æ£€æŸ¥ .env æ–‡ä»¶")
        return

    # æ˜¾ç¤º AI å“åº”ï¼ˆæµå¼ï¼‰
    with st.chat_message("assistant"):
        message_placeholder = st.empty()
        full_response = ""

        try:
            # æ„å»ºæ¶ˆæ¯åˆ—è¡¨
            messages = [
                {"role": msg["role"], "content": msg["content"]}
                for msg in st.session_state.messages[:-1]
            ]
            messages.append({"role": "user", "content": user_input})

            # è·å–å“åº”
            if use_rag and st.session_state.get("rag_service"):
                logger.info(f"Using RAG for query: {user_input[:50]}...")
                response_generator = st.session_state.rag_service.generate_response_with_rag(
                    user_input,
                    messages[:-1],
                    use_rag=True,
                    temperature=st.session_state.get("temperature", 0.7),
                    max_tokens=st.session_state.get("max_tokens", 2048),
                )
            else:
                logger.info(f"Using standard chat for query: {user_input[:50]}...")
                response_generator = st.session_state.deepseek_client.chat_stream(
                    messages=messages,
                    temperature=st.session_state.get("temperature", 0.7),
                    max_tokens=st.session_state.get("max_tokens", 2048),
                )

            # æµå¼æ˜¾ç¤ºå“åº”
            for chunk in response_generator:
                full_response += chunk
                message_placeholder.markdown(full_response + "â–Œ")

            # ç§»é™¤å…‰æ ‡
            message_placeholder.markdown(full_response)

            # æ·»åŠ  AI æ¶ˆæ¯åˆ°å†å²
            st.session_state.messages.append(
                {
                    "role": "assistant",
                    "content": full_response,
                    "timestamp": datetime.now().isoformat(),
                }
            )

            logger.info(f"Response generated, estimated tokens: {st.session_state.deepseek_client.count_tokens_estimate(full_response)}")

        except Exception as e:
            error_msg = f"âŒ å‘ç”Ÿé”™è¯¯: {str(e)}"
            message_placeholder.error(error_msg)
            logger.error(f"Error generating response: {str(e)}")


def upload_documents_to_knowledge_base(uploaded_files):
    """ä¸Šä¼ æ–‡æ¡£åˆ°çŸ¥è¯†åº“"""
    if not uploaded_files:
        return

    if not st.session_state.get("rag_service"):
        st.error("âŒ RAG æœåŠ¡æœªåˆå§‹åŒ–")
        return

    progress_bar = st.progress(0)
    status_text = st.empty()

    try:
        all_chunks = []
        all_metadata = []

        for idx, uploaded_file in enumerate(uploaded_files):
            status_text.text(f"å¤„ç†æ–‡ä»¶ {idx + 1}/{len(uploaded_files)}: {uploaded_file.name}")

            # ä¿å­˜ä¸´æ—¶æ–‡ä»¶
            temp_path = settings.DATA_DIR / "temp" / uploaded_file.name
            temp_path.parent.mkdir(parents=True, exist_ok=True)

            with open(temp_path, "wb") as f:
                f.write(uploaded_file.getbuffer())

            # å¤„ç†æ–‡æ¡£
            chunks, metadata = st.session_state.document_processor.process_file(str(temp_path))

            # æ·»åŠ æºæ–‡ä»¶ååˆ°å…ƒæ•°æ®
            for m in metadata:
                m["filename"] = uploaded_file.name

            all_chunks.extend(chunks)
            all_metadata.extend([{**m, "chunk_index": i} for i, _ in enumerate(chunks)])

            # åˆ é™¤ä¸´æ—¶æ–‡ä»¶
            os.remove(temp_path)

            progress_bar.progress((idx + 1) / len(uploaded_files))

        # æ·»åŠ åˆ°çŸ¥è¯†åº“
        if all_chunks:
            status_text.text(f"æ·»åŠ  {len(all_chunks)} ä¸ªæ–‡æœ¬å—åˆ°çŸ¥è¯†åº“...")
            st.session_state.rag_service.add_documents(all_chunks, all_metadata)
            status_text.text(f"âœ… æˆåŠŸæ·»åŠ  {len(all_chunks)} ä¸ªæ–‡æœ¬å—")
            logger.info(f"Successfully added {len(all_chunks)} chunks to knowledge base")
        else:
            status_text.text("âŒ æœªèƒ½ä»æ–‡ä»¶ä¸­æå–å†…å®¹")

    except Exception as e:
        status_text.error(f"âŒ é”™è¯¯: {str(e)}")
        logger.error(f"Error uploading documents: {str(e)}")


def main():
    """ä¸»å‡½æ•°"""
    initialize_session_state()

    # é¡µé¢æ ‡é¢˜
    st.title("ğŸ’¬ DeepSeek AI å¯¹è¯åŠ©æ‰‹ + RAG çŸ¥è¯†åº“")
    st.markdown("åŸºäº DeepSeek API å’Œ RAG çš„ AI å¯¹è¯åº”ç”¨ | é˜¶æ®µ 1 + é˜¶æ®µ 2")

    # ä¾§è¾¹æ 
    with st.sidebar:
        st.header("âš™ï¸ åº”ç”¨è®¾ç½®")

        # API çŠ¶æ€
        if st.session_state.deepseek_client is not None:
            st.success("âœ… DeepSeek API å·²è¿æ¥")
            st.caption(f"æ¨¡å‹: {st.session_state.deepseek_client.model}")
        else:
            st.error("âŒ DeepSeek API æœªè¿æ¥")

        # RAG çŠ¶æ€
        if st.session_state.get("rag_service"):
            st.success("âœ… RAG æœåŠ¡å·²å°±ç»ª")
            kb_info = st.session_state.rag_service.get_knowledge_base_info()
            st.caption(f"çŸ¥è¯†åº“æ–‡æ¡£æ•°: {kb_info['document_count']}")
        else:
            st.warning("âš ï¸ RAG æœåŠ¡æœªåˆå§‹åŒ–")

        st.divider()

        # å¯¹è¯å‚æ•°
        st.subheader("ğŸ’­ å¯¹è¯å‚æ•°")
        st.session_state.temperature = st.slider(
            "æ¸©åº¦ (Temperature)",
            min_value=0.0,
            max_value=2.0,
            value=0.7,
            step=0.1,
            help="æ›´é«˜çš„æ¸©åº¦ä¼šå¯¼è‡´æ›´å¤šçš„åˆ›æ„å“åº”",
        )

        st.session_state.max_tokens = st.slider(
            "æœ€å¤§ Token æ•°",
            min_value=256,
            max_value=4096,
            value=2048,
            step=256,
        )

        # RAG å¼€å…³
        st.divider()
        st.subheader("ğŸ” RAG çŸ¥è¯†åº“")
        use_rag = st.toggle(
            "å¯ç”¨ RAG çŸ¥è¯†åº“",
            value=True,
            help="å¯ç”¨åå°†ä½¿ç”¨çŸ¥è¯†åº“ä¸­çš„æ–‡æ¡£æ¥å¢å¼º AI å›å¤",
        )

        st.session_state.use_rag = use_rag

        st.divider()

        # å¯¹è¯ç®¡ç†
        st.subheader("ğŸ“‹ å¯¹è¯ç®¡ç†")
        col1, col2 = st.columns(2)

        with col1:
            message_count = len([m for m in st.session_state.messages if m["role"] == "user"])
            st.metric("æ¶ˆæ¯æ•°é‡", message_count)

        with col2:
            if st.session_state.get("deepseek_client"):
                total_tokens = st.session_state.deepseek_client.count_messages_tokens(
                    [{"role": m["role"], "content": m["content"]} for m in st.session_state.messages]
                )
                st.metric("ä¼°è®¡ Token", total_tokens)

        if st.button("ğŸ—‘ï¸ æ¸…ç©ºå¯¹è¯å†å²", use_container_width=True):
            st.session_state.messages = []
            st.success("âœ… å¯¹è¯å†å²å·²æ¸…ç©º")
            st.rerun()

    # ä¸»åŒºåŸŸä½¿ç”¨ Tab
    tab1, tab2 = st.tabs(["ğŸ’¬ å¯¹è¯", "ğŸ“š çŸ¥è¯†åº“ç®¡ç†"])

    # Tab 1: å¯¹è¯
    with tab1:
        # æ˜¾ç¤ºå¯¹è¯å†å²
        display_chat_history()

        # ç”¨æˆ·è¾“å…¥
        if user_input := st.chat_input(
            "è¾“å…¥ä½ çš„é—®é¢˜... (æŒ‰ Enter å‘é€)",
            key="chat_input",
        ):
            handle_user_input(user_input, use_rag=st.session_state.get("use_rag", True))

    # Tab 2: çŸ¥è¯†åº“ç®¡ç†
    with tab2:
        st.subheader("ğŸ“š çŸ¥è¯†åº“ç®¡ç†")

        if not st.session_state.get("rag_service"):
            st.error("âŒ RAG æœåŠ¡æœªåˆå§‹åŒ–ï¼Œæ— æ³•ç®¡ç†çŸ¥è¯†åº“")
        else:
            # çŸ¥è¯†åº“ä¿¡æ¯
            kb_info = st.session_state.rag_service.get_knowledge_base_info()

            col1, col2, col3 = st.columns(3)
            with col1:
                st.metric("æ–‡æ¡£æ•°é‡", kb_info["document_count"])
            with col2:
                st.metric("æ£€ç´¢ Top-K", kb_info["top_k"])
            with col3:
                st.metric("çŠ¶æ€", kb_info["status"])

            st.divider()

            # ä¸Šä¼ æ–‡æ¡£
            st.subheader("ğŸ“„ ä¸Šä¼ æ–‡æ¡£")
            uploaded_files = st.file_uploader(
                "é€‰æ‹©æ–‡ä»¶ï¼ˆæ”¯æŒ PDFã€Wordã€TXTï¼‰",
                accept_multiple_files=True,
                type=["pdf", "docx", "doc", "txt"],
                help="æ”¯æŒçš„æ ¼å¼: PDF, Word (.docx, .doc), Text (.txt)",
            )

            if st.button("æ·»åŠ åˆ°çŸ¥è¯†åº“", use_container_width=True):
                if uploaded_files:
                    upload_documents_to_knowledge_base(uploaded_files)
                else:
                    st.warning("è¯·å…ˆé€‰æ‹©æ–‡ä»¶")

            st.divider()

            # æ¸…ç©ºçŸ¥è¯†åº“
            st.subheader("âš ï¸ å±é™©æ“ä½œ")
            if st.button("ğŸ—‘ï¸ æ¸…ç©ºçŸ¥è¯†åº“", use_container_width=True, type="secondary"):
                try:
                    st.session_state.rag_service.clear_knowledge_base()
                    st.success("âœ… çŸ¥è¯†åº“å·²æ¸…ç©º")
                    st.rerun()
                except Exception as e:
                    st.error(f"âŒ é”™è¯¯: {str(e)}")

    # åº•éƒ¨ä¿¡æ¯
    st.divider()
    st.caption(
        f"ç‰ˆæœ¬: 0.2.0 | é˜¶æ®µ 1 + é˜¶æ®µ 2 | æœ€åæ›´æ–°: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}"
    )


if __name__ == "__main__":
    main()
